{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "In Convolutional Neural Networks (CNNs), a receptive field refers to the portion of the input image that a single neuron in a layer is \"looking at\". Each neuron's receptive field is determined by the size of its convolutional kernel, the number of layers in the network, and the stride with which the kernel moves across the input image. The receptive field grows with each subsequent layer, as each neuron receives input from a larger region of the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "There are several ways to control the rate at which the receptive field grows from layer to layer in CNNs.\n",
    "The first approach is to use smaller convolutional kernels (such as 3x3) and increase the number of layers in the network. This approach is called \"deepening\" and has the advantage of increasing the non-linearity of the network, since each layer introduces a non-linear activation function. By using smaller kernels, the receptive field grows more slowly, but more layers are needed to cover the same region of the input image.\n",
    "\n",
    "The second approach is to use pooling layers between the convolutional layers. Pooling layers reduce the spatial dimensionality of the input, typically by taking the maximum or average value over a small region (such as 2x2) of the previous layer. This has the effect of increasing the receptive field of each neuron in the next layer, since they are now looking at a larger region of the input. However, pooling layers also reduce the resolution of the input, which can result in loss of information.\n",
    "\n",
    "The third approach is to use dilated convolutions, also known as atrous convolutions. Dilated convolutions insert gaps between the values in the convolutional kernel, effectively increasing the size of the kernel without increasing the number of parameters. This has the effect of increasing the receptive field of each neuron, while still maintaining a high spatial resolution of the input. However, dilated convolutions can result in a more sparse representation of the input, which may reduce the performance of the network.\n",
    "\n",
    "In terms of how they combine input features, deepening and dilated convolutions both combine input features in a local, dense manner. Pooling, on the other hand, combines features in a more global, sparse manner by taking the maximum or average value over a larger region of the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:\n",
    "\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "The CNN with three convolutional layers can be defined as follows:\n",
    "Layer 1: 32 filters with a 3x3 kernel, ReLU activation, and padding\n",
    "Layer 2: 64 filters with a 3x3 kernel, ReLU activation, and padding\n",
    "Layer 3: 128 filters with a 3x3 kernel, ReLU activation, and padding\n",
    "In layer 1, each filter will have a receptive field of 3x3 pixels, meaning each neuron is looking at a 3x3 patch of the input. In layer 2, each neuron will have a receptive field of 5x5 pixels, since it receives input from a 3x3 patch of the previous layer. Finally, in layer 3, each neuron will have a receptive field of 7x7 pixels, since it receives input from a 3x3 patch of the previous layer.\n",
    "\n",
    "To interpret the performance of the network, one would need to consider the dataset being used, the objective of the task, and the evaluation metrics being used. However, in general, deeper networks with larger receptive fields tend to perform better on tasks that require a high degree of spatial abstraction, such as object recognition or semantic segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:16.753322Z",
     "iopub.status.busy": "2021-01-26T09:18:16.752610Z",
     "iopub.status.idle": "2021-01-26T09:18:17.987501Z",
     "shell.execute_reply": "2021-01-26T09:18:17.988034Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    " The sizeof the receptive field of each \"pixel\" in the output tensor of this CNN is 112x112 pixels.\n",
    "\n",
    "The receptive field of a neuron in a CNN represents the region in the input image that affects the activation of that neuron. The receptive field size of each neuron in the output feature map of a CNN depends on the size of the convolutional kernels and the pooling layers used in the network.\n",
    "\n",
    "In this CNN, the receptive field size of each neuron in the output feature map is calculated as follows:\n",
    "\n",
    "First convolutional layer: Kernel size = 3x3, Padding = 1, resulting in a receptive field of size 3x3.\n",
    "First max-pooling layer: Kernel size = 2x2, Stride = 2, resulting in a receptive field of size 6x6.\n",
    "Second convolutional layer: Kernel size = 5x5, Stride = 2, Padding = 2, resulting in a receptive field of size 16x16.\n",
    "Second max-pooling layer: Kernel size = 2x2, Stride = 2, resulting in a receptive field of size 32x32.\n",
    "Third convolutional layer: Kernel size = 7x7, Dilation = 2, Padding = 3, resulting in a receptive field of size 112x112.\n",
    "Therefore, the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor of this CNN is 112x112 pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this.\n",
    "\n",
    "//======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "\n",
    "The reason for the observed differences in learned filters between the original CNN and the residual CNN lies in the way the residual connections affect the optimization process. In the original CNN, each layer is optimized to produce the desired output directly from its input, without any shortcuts or additional inputs. In the residual CNN, each layer is optimized to produce the desired output by adding the input to the result of its convolutional operation. This means that the optimization process in the residual CNN can take advantage of the residual connections to skip over difficult regions of the optimization landscape, and focus on learning more complex and meaningful representations. As a result, the learned filters in the residual CNN may be more diverse, specialized, and effective than those in the original CNN, since they can leverage both the input information and the residual information to improve their performance. However, this also means that the learned filters in the residual CNN may not be directly comparable or interpretable with those in the original CNN, since they represent different optimization objectives and strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "In order to replace the two consecutive dropout layers with a single one, we need to find the equivalent drop probability q that would have the same effect as applying p1 and p2 consecutively. This can be computed as follows:\n",
    "\n",
    "q = 1 - (1 - p1) * (1 - p2)\n",
    "\n",
    "The idea behind this calculation is that the probability of keeping each unit active after applying two consecutive dropout layers with probabilities p1 and p2 is equal to the product of the individual keep probabilities, which is equivalent to the probability ,of dropping out each unit with probability q = 1 - keep_prob,In our case, q=0.28.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "False. Dropout can be placed both before or after the activation function, depending on the specific architecture and objectives of the neural network. In general, placing dropout before the activation function may be more effective in preventing overfitting, since it can reduce the co-adaptation between units and encourage more diverse and robust representations. However, placing dropout after the activation function may also be beneficial in some cases, since it can allow the units to learn more complex and expressive transformations without being excessively constrained by the dropout mask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "After applying dropout with a drop-probability of p, the activations are scaled by 1/(1-p) in order to maintain their expected value unchanged. To see why this is the case, consider a single activation a that is either kept with probability 1-p or set to zero with probability p. The expected value of this activation can be computed as follows:\n",
    "\n",
    "E[a] = (1-p) * a + p * 0 = (1-p) * a\n",
    "\n",
    "If we want to maintain the expected value of a unchanged after applying dropout, we need to scale it by 1/(1-p) to compensate for the reduction in the number of active units. This means that the actual activation a' after dropout will be given by:\n",
    "\n",
    "a' = a * mask / (1-p)\n",
    "\n",
    "where mask is a binary mask that determines which units are kept and which are dropped. By multiplying a by mask, we set the dropped units to zero and keep the active units unchanged, while the scaling factor of 1/(1-p) ensures that the expected value of a' is equal to the expected value of a before dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "No, an L2 loss is not appropriate for training a binary classifier like the one described here. The L2 loss, also known as the mean squared error (MSE), is a regression loss function that measures the average squared difference between the predicted and target values. It is commonly used for problems where the output is a continuous variable, such as predicting a numeric value or a probability. However, for a binary classification problem like this, the output is a discrete variable with only two possible values, so using a regression loss like L2 would not be suitable.\n",
    "The reason that using L2 loss is not appropriate for binary classification problems is that the output of the model is a probability distribution over the classes (in this case, dog and hotdog), rather than a continuous value. L2 loss is designed for continuous output values, and it tries to minimize the difference between the predicted and true values by penalizing the squared differences.\n",
    "\n",
    "In binary classification, a common loss function to use is the binary cross-entropy loss, also known as log loss. This loss function is designed to measure the difference between two probability distributions, in this case, the predicted probability distribution and the true probability distribution. The binary cross-entropy loss works by taking the negative log likelihood of the predicted probability of the correct class.\n",
    "\n",
    "Here's an example to illustrate why L2 loss is not appropriate for binary classification. Let's say we have a model that outputs a probability distribution over the classes, and we want to classify an image as either a cat (output 0) or a dog (output 1). The true label for an image is a dog, so the true probability distribution is [0, 1].\n",
    "\n",
    "If we train the model with L2 loss, and the model outputs [0.5, 0.5], the L2 loss would be (0.5-0)^2 + (0.5-1)^2 = 0.5. However, this does not reflect the fact that the model is uncertain and doesn't strongly predict either class. In contrast, the binary cross-entropy loss would penalize the model for being uncertain and not strongly predicting the true class.\n",
    "\n",
    "Instead, we can use a binary cross-entropy loss, which is a commonly used loss function for binary classification problems. The binary cross-entropy loss measures the difference between the predicted probability and the target probability for a binary classification problem. It is defined as:\n",
    "\n",
    "L = -[y*log(p) + (1-y)*log(1-p)]\n",
    "\n",
    "\n",
    "\n",
    "where y is the ground-truth label (0 for dog and 1 for hotdog), p is the predicted probability of the positive class (hotdog), and log is the natural logarithm.\n",
    "\n",
    "To illustrate the difference between L2 and binary cross-entropy losses, consider a simple example where we have two training examples and their corresponding true labels and model predictions:\n",
    "\n",
    "example\tTrue Label\tModel Prediction\n",
    "  1\t        0\t         0.8\n",
    "  2     \t  1\t         0.2\n",
    "\n",
    "\n",
    "If we use an L2 loss to train the model, the loss would be computed as the mean squared error between the true labels and the predicted values:\n",
    "\n",
    "L2 loss = (0 - 0.8)^2 + (1 - 0.2)^2 = 1.16\n",
    "\n",
    " This loss does not reflect the fact that the model is making correct predictions for both examples, but is just not confident in its predictions. Using an L2 loss could lead the model to assign equal weights to both examples, which could result in suboptimal performance.\n",
    "\n",
    "On the other hand, if we use a binary cross-entropy loss to train the model, the loss would be computed as follows:\n",
    "\n",
    "Binary cross-entropy loss =\n",
    " -[0*log(0.8) + (1-0)*log(1-0.8)] - [1*log(0.2) + (1-1)*log(1-0.2)] = 0.965\n",
    "\n",
    " This loss penalizes the model more for making incorrect predictions and rewards it more for making correct predictions with higher confidence. It is a more suitable loss function for binary classification problems.\n",
    "\n",
    "In summary, L2 loss is not appropriate for binary classification problems because the output of the model is a probability distribution over the classes, and L2 loss is designed for continuous output values. Instead, binary cross-entropy loss is a better choice because it measures the difference between two probability distributions and penalizes the model for being uncertain and not strongly predicting the true class.\n",
    "\n",
    "Therefore, we should use a binary cross-entropy loss to train a binary classifier like the one described in the problem statement.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/PiratesVsTemp%28en%29.svg/1200px-PiratesVsTemp%28en%29.svg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "The most likely cause for the plateau in loss after only a few iterations is the vanishing gradient problem. This problem arises when gradients in the backpropagation algorithm become too small to effectively update the weights in the earlier layers of the network. As a result, the weights in these layers remain largely unchanged, leading to a stagnant or plateauing training process.\n",
    "\n",
    "In the given model, the repeated use of the Sigmoid activation function may be causing the vanishing gradient problem. The Sigmoid function has a maximum gradient of 0.25, which means that as backpropagation proceeds through the layers of the network, the gradients can become exponentially small. This makes it difficult to update the weights in the earlier layers of the network, and can lead to a plateau in training.\n",
    "\n",
    "To address this issue, one potential solution is to use an activation function with a larger maximum gradient, such as the Rectified Linear Unit (ReLU). Another solution could be to use normalization techniques, such as Batch Normalization or Layer Normalization, which help stabilize the gradient flow through the network.\n",
    "\n",
    "In addition, the architecture of the given model may be too deep, with 24 hidden layers. Deep neural networks are more prone to the vanishing gradient problem, especially when using certain activation functions. In this case, reducing the number of layers or using skip connections (such as in a ResNet architecture) could help alleviate the issue. \n",
    "\n",
    "Overall, the vanishing gradient problem is a well-known challenge in deep learning, and addressing it requires careful consideration of the model architecture and the activation functions used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "Is it correct to replace the sigmoid activations with tanh to solve the problem? Explain why or why not.\n",
    "Replacing the sigmoid activations with tanh may or may not solve the problem of the plateau in loss during training. The tanh activation function is similar to sigmoid in that it is also sigmoidal in shape, but it is centered around zero and ranges from -1 to 1, instead of 0 to 1. The advantage of tanh over sigmoid is that it can output negative values, which can be useful in some situations.\n",
    "\n",
    "However, in this case, the choice of activation function is unlikely to be the primary cause of the plateau in loss during training. The mlpirate model has a large number of layers (25), and the repeated use of the same activation function can lead to the saturation of the gradients. This saturation can cause the gradients to vanish or explode, making it difficult for the optimization algorithm to update the weights effectively. Therefore, replacing the activation function may not be sufficient to overcome this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "In a model using exclusively ReLU activations, there can be no vanishing gradients; The gradient of ReLU is linear with its input when the input is positive; ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero.\n",
    "\n",
    "a) False.\n",
    " While ReLU activations are known to alleviate the problem of vanishing gradients, they can still occur in deep networks that use exclusively ReLU activations. When the input to a ReLU activation is negative, the gradient is zero, which can cause the gradients to vanish during backpropagation.\n",
    "\n",
    "b) True. \n",
    "When the input to a ReLU activation is positive, the gradient is equal to 1, which means that the gradient is linear with respect to its input.\n",
    "\n",
    "c) True. \n",
    "ReLU can cause \"dead\" neurons, which are neurons that always output zero, regardless of the input. This can happen if the bias term is set such that the weighted input is always negative. In this case, the gradient of the neuron is always zero, and the neuron remains inactive. Dead neurons can significantly reduce the capacity of a neural network and are often a problem in deep networks. One way to address this issue is to use variants of ReLU, such as leaky ReLU or ELU, which\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD).\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize the objective function in a neural network. In SGD, at each iteration, the gradient of the objective function is estimated using a randomly selected sample from the training data. This allows the algorithm to converge more quickly than regular gradient descent, as the gradient update is performed more frequently.\n",
    "\n",
    "Mini-batch SGD\n",
    "Mini-batch SGD is a variation of stochastic gradient descent where the gradient is estimated using a small batch of randomly selected samples from the training data. This method strikes a balance between the high variance of stochastic gradient descent and the high computational cost of regular gradient descent. By using a batch of samples, the estimate of the gradient is less noisy than in stochastic gradient descent, but it is still more frequent than in regular gradient descent.\n",
    "\n",
    "Regular Gradient Descent (GD)\n",
    "Regular Gradient Descent (GD) is an optimization algorithm that updates the parameters of a model using the gradient of the objective function computed on the entire training set. GD computes the gradient of the objective function with respect to the model parameters using the entire dataset, which can be computationally expensive and slow. As a result, it is not suitable for large datasets and high-dimensional models.\n",
    "\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1. \n",
    "\n",
    "  A) Computational efficiency: SGD is more computationally efficient than GD because it requires less memory and fewer computations per iteration. In SGD, only a small subset of the training data is used to estimate the gradient at each iteration, which reduces the computational cost.\n",
    "\n",
    "  B) Robustness to noise: SGD is less sensitive to noise in the data than GD. The reason for this is that in SGD, the gradient is estimated using a random subset of the training data, which results in a noisy estimate of the true gradient. However, this noise can be beneficial because it can help the algorithm avoid getting stuck in local optima and escape saddle points.\n",
    "\n",
    "2.\n",
    " \n",
    " GD cannot be used when the dataset is too large to fit into memory, or when the model is too complex to compute the gradient on the entire dataset. GD requires the computation of the gradient on the entire dataset, which can be computationally infeasible for large datasets or high-dimensional models. In such cases, SGD or mini-batch SGD are more appropriate.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "When training a deep neural network using mini-batch SGD, the batch size is an important hyperparameter that affects the convergence speed and the quality of the solution. A larger batch size results in a more accurate estimate of the gradient, but at the cost of slower convergence. Conversely, a smaller batch size leads to a noisier estimate of the gradient but a faster convergence.\n",
    "\n",
    "In the given scenario, the model was trained using mini-batch SGD with a batch size of B, and it converged to a loss value of l0 within n iterations on average. The goal is to determine the effect of increasing the batch size from B to 2B on the convergence speed.\n",
    "\n",
    "When the batch size is doubled from B to 2B, the number of iterations required to converge to l0 is expected to increase. This is because larger batch sizes result in a more accurate estimate of the gradient, but at the cost of slower convergence. In other words, increasing the batch size results in a more stable and accurate estimate of the gradient, but it also reduces the stochasticity of the algorithm, resulting in a smoother trajectory towards the optimum.\n",
    "\n",
    "To illustrate this, consider the following example. Suppose we have a dataset of 10,000 samples, and we are training a neural network with mini-batch SGD using a learning rate of 0.001. We run the experiment with two different batch sizes: 32 and 64.\n",
    "\n",
    "When using a batch size of 32, the algorithm updates the parameters after each batch of 32 samples. Therefore, it takes 3125 iterations (i.e., 10,000/32) to process the entire dataset. On the other hand, when using a batch size of 64, the algorithm updates the parameters after each batch of 64 samples. Therefore, it takes 1563 iterations (i.e., 10,000/64) to process the entire dataset. As we can see, doubling the batch size results in half as many iterations required to process the same amount of data.\n",
    "\n",
    "However, this does not necessarily mean that the convergence speed will be faster with a larger batch size. In fact, the opposite is often true, as larger batch sizes reduce the stochasticity of the algorithm, resulting in a smoother trajectory towards the optimum. Therefore, it is important to choose the batch size carefully to balance the trade-off between convergence speed and accuracy of the gradient estimate.\n",
    "\n",
    "In summary, when increasing the batch size from B to 2B, the number of iterations required to converge to l0 is expected to increase due to the reduced stochasticity of the algorithm. However, the final quality of the solution may improve due to the more accurate estimate of the gradient. Therefore, the choice of batch size should be based on the specific requirements of the problem at hand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum.\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "\n",
    "1) False.\n",
    "In SGD, we perform an optimization step for each mini-batch of samples, not for each individual sample. For example, if we have a dataset with 100,000 samples and a batch size of 100, then each epoch would consist of 1,000 optimization steps, not 100,000. Performing an optimization step for each sample would be computationally expensive and not feasible for large datasets. Instead, we sample mini-batches of data and perform optimization steps on these mini-batches.\n",
    "\n",
    "2) False.\n",
    "Gradients obtained with SGD have more variance compared to GD because they're computed on a smaller number of samples. In GD, we compute the gradient on the entire dataset, which can be noisy and time-consuming for large datasets. In contrast, SGD computes the gradient on a small subset of the dataset (i.e., a mini-batch), which increases the noise in the gradient estimate and can slow down convergence. However, SGD samples the parameter space more frequently, which helps it explore different directions and potentially find better solutions.\n",
    "\n",
    "3) True.\n",
    "SGD is less likely to get stuck in local minima compared to GD because it introduces more randomness in the optimization process by sampling different mini-batches in each iteration. This allows the algorithm to escape from poor local minima and potentially reach better solutions. In contrast, GD can get stuck in poor local minima because it always moves in the direction of the steepest descent, which can lead it to converge to suboptimal solutions.\n",
    "\n",
    "4) True.\n",
    "Training with SGD requires less memory than GD because we only need to keep a small subset of the dataset (the mini-batch) in memory at each iteration, while in GD, we need to keep the entire dataset in memory to compute the gradient. For example, if we have a dataset with 100,000 samples and a batch size of 100, then each mini-batch would only contain 100 samples, which is much smaller than the entire dataset. Therefore, SGD is more memory-efficient than GD.\n",
    "\n",
    "5) False.\n",
    "Neither SGD nor GD are guaranteed to converge to a global minimum, but they're both guaranteed to converge to a stationary point, which could be a local minimum, a saddle point, or a global minimum. The convergence behavior depends on the properties of the loss function and the optimization algorithm used. However, SGD has a better chance of escaping poor local minima due to its stochasticity.\n",
    "\n",
    "6) True.\n",
    "SGD with momentum is more likely to converge more quickly than Newton's method which doesn't have momentum in a narrow ravine. A narrow ravine is a region of the loss surface with high curvature in one direction and low curvature in the perpendicular direction. In this case, the gradient descent direction is mostly aligned with the high curvature direction, which can cause oscillations and slow convergence. SGD with momentum adds a fraction of the previous update to the current update, which allows it to move faster in the direction of the gradient and dampen oscillations caused by the high curvature. In contrast, Newton's method uses the second-order information of the loss function to compute the direction of descent, which could be slow to adapt to the narrow ravine. Therefore, SGD with momentum is more suitable for optimizing functions with narrow ravines than Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "False. It is not necessary to use a descent-based method to solve the inner optimization problem in bi-level optimization. In fact, the inner optimization problem can be solved using any optimization algorithm that can find a stationary point of the problem, such as interior-point methods, trust-region methods, or even gradient-free methods.\n",
    "\n",
    "The reason for this is that the outer optimization problem, which is typically solved using a descent-based method such as SGD, provides a gradient-based approximation of the bi-level problem. This approximation is based on the assumption that the inner optimization problem has a unique and stationary solution for each choice of the outer optimization variables. If this assumption holds, then the gradient of the outer objective function with respect to the outer optimization variables can be expressed in terms of the gradient of the inner objective function with respect to the inner optimization variables. This is known as the implicit function theorem.\n",
    "\n",
    "Therefore, as long as the inner optimization problem has a unique and stationary solution for each choice of the outer optimization variables, any optimization algorithm that can find such a solution can be used to solve the inner problem. However, it is important to note that some optimization algorithms may be more efficient or better suited for certain types of problems than others, and the choice of algorithm may affect the overall performance of the bi-level optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1.\n",
    "  \"Vanishing gradients\" and \"exploding gradients\" are two common problems that can arise when training deep neural networks. Vanishing gradients refer to the phenomenon where the gradients of the loss function with respect to the parameters of the earlier layers in the network become very small, making it difficult for the network to learn useful representations of the input. Exploding gradients, on the other hand, refer to the opposite phenomenon, where the gradients become very large and cause the optimization algorithm to diverge.\n",
    "\n",
    "2.  \n",
    "  Both of these problems can be caused by increased depth in the neural network. When the network is deep, gradients must be propagated through multiple layers, which can amplify or dampen the gradients depending on the magnitude of the weights and activation functions in each layer. In particular, vanishing gradients tend to occur when the weights in the network are small, and the activation function is such that its derivatives are also small, which causes the gradients to become very small as they propagate through the network. Exploding gradients, on the other hand, tend to occur when the weights are large, and the activation function is such that its derivatives are also large, causing the gradients to become very large as they propagate through the network.\n",
    "\n",
    "3.\n",
    "  To illustrate vanishing gradients, consider a deep neural network with 10 layers, where each layer has a weight matrix with entries drawn from a Gaussian distribution with mean 0 and standard deviation 1. Let the activation function be the sigmoid function. We can generate a random input vector $\\vec{x}$ and compute the gradients of the output with respect to the parameters of the first layer using backpropagation. If we repeat this process many times, we may observe that the magnitude of the gradients becomes smaller and smaller as we move further back in the network, making it difficult for the network to learn useful representations of the input.\n",
    "\n",
    "  To illustrate exploding gradients, we can use a similar network architecture, but with weight matrices whose entries are drawn from a Gaussian distribution with mean 0 and standard deviation 100. Again, let the activation function be the sigmoid function. If we compute the gradients of the output with respect to the parameters of the first layer using backpropagation, we may observe that the magnitude of the gradients becomes very large as we move further back in the network, causing the optimization algorithm to diverge.\n",
    "\n",
    "4.\n",
    "  If we suspect that either vanishing or exploding gradients is occurring in our network, we can look at the distribution of the gradients during training to determine which problem is present. If the gradients tend to become very small as we move further back in the network, then we may be experiencing vanishing gradients. Conversely, if the gradients tend to become very large, then we may be experiencing exploding gradients. Additionally, we can monitor the loss function during training to see if it is decreasing or increasing, as this can also provide information about the stability of the optimization algorithm. If the loss function is increasing, then it is likely that we are experiencing exploding gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "ANSWER:\n",
    "\n",
    "To calculate the derivatives of $L_{\\mathcal{S}}$ w.r.t. the model parameters, we can use the chain rule of differentiation. The chain rule states that the derivative of the composition of two functions is equal to the product of their derivatives. In the case of neural networks, the composition of functions refers to the forward pass through the layers of the network.\n",
    "\n",
    "We begin by calculating the derivative of the loss function with respect to the predicted output $\\hat{y}^{(i)}$. Using the chain rule and the definition of binary cross-entropy loss, we have:\n",
    "\n",
    "\n",
    "$\\mat{W}_2$:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\frac{\\partial L_{\\mathcal{S}}}{\\partial \\mat{W}2} &= \\sum{i=1}^m \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\hat{y}^{(i)}} \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{z}^{(i)}_2} \\frac{\\partial \\vec{z}^{(i)}_2}{\\partial \\mat{W}2} \\\n",
    "&= \\sum{i=1}^m \\frac{1}{m} \\cdot \\frac{\\hat{y}^{(i)} - y^{(i)}}{\\hat{y}^{(i)} (1 - \\hat{y}^{(i)})} \\cdot \\hat{y}^{(i)} (1 - \\hat{y}^{(i)}) \\vec{a}^{(i)}1 \\\n",
    "&= \\frac{1}{m} \\sum{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\vec{a}^{(i)}_1\n",
    "\\end{align*}$\n",
    "\n",
    "where we used the fact that $\\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{z}^{(i)}_2} = \\hat{y}^{(i)} (1 - \\hat{y}^{(i)})$ and $\\frac{\\partial \\vec{z}^{(i)}_2}{\\partial \\mat{W}_2} = \\vec{a}^{(i)}_1$.\n",
    "\n",
    "Finally, we can compute the derivative of the loss with respect to the bias vector of the first hidden layer $\\vec{b}_1$ using the chain rule:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\frac{\\partial L_{\\mathcal{S}}}{\\partial \\vec{b}1} &= \\sum{i=1}^m \\frac{\\partial L_{\\mathcal{S}}}{\\partial \\hat{y}^{(i)}} \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\vec{z}^{(i)}_2} \\frac{\\partial \\vec{z}^{(i)}_2}{\\partial \\vec{a}^{(i)}_1} \\frac{\\partial \\vec{a}^{(i)}_1}{\\partial \\vec{z}^{(i)}_1} \\frac{\\partial \\vec{z}^{(i)}_1}{\\partial \\vec{b}1} \\\n",
    "&= \\sum{i=1}^m \\frac{1}{m} \\cdot \\frac{\\hat{y}^{(i)} - y^{(i)}}{\\hat{y}^{(i)} (1 - \\hat{y}^{(i)})} \\cdot \\mat{W}_2^\\top \\hat{y}^{(i)} (1 - \\hat{y}^{(i)}) \\cdot \\sigma'(\\vec{z}1^{(i)}) \\\n",
    "&= \\frac{1}{m} \\sum{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\mat{W}_2^\\top \\cdot \\sigma'(\\vec{z}_1^{(i)})\n",
    "\\end{align*}$\n",
    "======================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD.\n",
    "\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "\n",
    "1.\n",
    "  The formula for computing the derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ can be used to compute gradients of neural network parameters numerically, without automatic differentiation (AD). In particular, suppose we have a neural network with parameters $\\theta$ and a loss function $L(\\theta)$ that we want to minimize. To compute the gradient of the loss with respect to the parameters, we can use the following numerical approximation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} L(\\theta) \\approx \\frac{1}{\\epsilon} \\sum_{i=1}^{m} [L(\\theta + \\epsilon\\vec{e}_i) - L(\\theta)] \\vec{e}_i\n",
    "$$\n",
    "\n",
    "  where $\\epsilon$ is a small positive scalar (known as the step size or perturbation parameter), $\\vec{e}_i$ is the $i$-th standard basis vector, and $m$ is the dimensionality of the parameter space.\n",
    "\n",
    "  Intuitively, the above formula computes the gradient of the loss by perturbing each parameter in turn and measuring the change in the loss. The gradient is then approximated as the sum of these changes, scaled by the step size.\n",
    "\n",
    "  This approach is known as finite difference approximation, and it is a simple way to compute gradients numerically. It can be used when automatic differentiation is not available (for example, when using a custom loss function), or as a way to check the correctness of gradients computed using AD.\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "There are several drawbacks of using finite difference approximation to compute gradients compared to automatic differentiation:\n",
    "\n",
    "Computational cost: Computing gradients using finite difference requires evaluating the loss function multiple times (once for each parameter), which can be computationally expensive. On the other hand, automatic differentiation can compute gradients with the same cost as computing the loss function itself.\n",
    "\n",
    "Numerical precision: The finite difference approximation involves subtracting two values that are close to each other, which can lead to numerical precision issues (e.g., cancellation of digits). This can be mitigated by using a smaller step size, but this increases the computational cost.\n",
    "\n",
    "Accuracy: The finite difference approximation is only an approximation, and its accuracy depends on the choice of the step size. Choosing a step size that is too small can lead to numerical precision issues, while choosing a step size that is too large can result in inaccurate gradients.\n",
    "\n",
    "Overall, while finite difference approximation can be a useful tool for computing gradients numerically, it is generally less efficient and less accurate than automatic differentiation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.049487Z",
     "iopub.status.busy": "2021-01-26T09:18:18.048852Z",
     "iopub.status.idle": "2021-01-26T09:18:18.072993Z",
     "shell.execute_reply": "2021-01-26T09:18:18.073616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.2914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    \"\"\"\n",
    "    Computes the mean of the dot product of X and W, plus b.\n",
    "\n",
    "    Args:\n",
    "        W (torch.Tensor): A tensor of shape (d, d) containing weights.\n",
    "        b (torch.Tensor): A tensor of shape (d,) containing biases.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor containing the mean of X @ W + b.\n",
    "    \"\"\"\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "\n",
    "\n",
    "def numerical_grad(loss_fn, param, eps=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the numerical gradient of a given loss function with respect to a given parameter using the central difference method.\n",
    "\n",
    "    Args:\n",
    "        loss_fn (callable): A function that computes the loss.\n",
    "        param (torch.Tensor): A tensor containing the parameter for which to compute the gradient.\n",
    "        eps (float): The step size to use for the central difference method.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of the same shape as param containing the numerical gradient of the loss with respect to param.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        indices = torch.arange(param.numel())\n",
    "        grad = torch.zeros_like(param)\n",
    "        for idx in indices:\n",
    "            # Calculate the numerical gradient of loss w.r.t. param[idx]\n",
    "            orig_val = param.view(-1)[idx].item()\n",
    "            param_copy = param.clone()\n",
    "            param_copy.view(-1)[idx] = orig_val + eps  \n",
    "            pos_loss = loss_fn()\n",
    "            param_copy.view(-1)[idx] = orig_val - eps\n",
    "            neg_loss = loss_fn()\n",
    "            grad.view(-1)[idx] = (pos_loss - neg_loss) / (2 * eps)\n",
    "        return grad\n",
    "\n",
    "grad_W = numerical_grad(loss_fn=lambda: foo(W, b), param=W)\n",
    "grad_b = numerical_grad(loss_fn=lambda: foo(W, b), param=b)\n",
    "\n",
    "autograd_W, autograd_b = torch.autograd.grad(loss, (W, b))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "A.\n",
    "  Word embeddings are a way to represent words in a low-dimensional space. In natural language processing, word embeddings are commonly used to represent words in a way that captures the semantic meaning of words.\n",
    "\n",
    "  Word embeddings are used in the context of a language model because they allow a language model to represent words in a way that is useful for prediction tasks. When training a language model, the model learns to predict the likelihood of a sequence of words. By using word embeddings, the language model can better understand the relationships between words in a sentence or document, which can improve the quality of its predictions.\n",
    "\n",
    "  For example, consider the sentence \"The cat sat on the mat\". Without word embeddings, a language model would represent each word as a one-hot vector, which is a sparse and high-dimensional representation. With word embeddings, the model can represent each word as a low-dimensional vector that captures its semantic meaning. This can help the model understand that \"cat\" and \"mat\" are related because they are both objects that are commonly found together.\n",
    "\n",
    "B.\n",
    "  Yes, a language model like the sentiment analysis example from the tutorials can be trained without using word embeddings. In this case, the model would be trained directly on sequences of tokens, such as words or characters.\n",
    "\n",
    "  The consequence of training a language model without using word embeddings is that the model may not be able to capture the semantic meaning of words or understand the relationships between words in a sentence. This could result in lower accuracy and performance on prediction tasks, especially for more complex tasks such as natural language understanding or generation. Additionally, training a language model without word embeddings may require a larger amount of training data to achieve similar performance compared to using word embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. \n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "A.  \n",
    "  The Y tensor contains the embedding of the input tensor X using the nn.Embedding module. The output shape of Y is (5, 6, 7, 8, 42000), where 5 is the batch size, 6 is the sequence length, 7 and 8 are the dimensions of the input tensor, and 42000 is the size of the embedding dimension.\n",
    "\n",
    "  The nn.Embedding module is used to map integer-encoded inputs to dense vectors, also known as embeddings. In this case, X is a tensor of shape (5, 6, 7, 8) that contains integer-encoded inputs representing words or tokens. The nn.Embedding module maps these integer-encoded inputs to dense vectors of size 42000, resulting in a tensor of shape (5, 6, 7, 8, 42000).\n",
    "\n",
    "B.\n",
    "  To implement nn.Embedding using only torch tensors, we can create a tensor embedding_weights of shape (num_embeddings, embedding_dim) to represent the embedding weights. We can then use the torch.nn.functional.embedding function to perform the embedding lookup.\n",
    "  Here is an example implementation:\n",
    "\n",
    "import torch\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self\n",
    "======================================================================\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.077958Z",
     "iopub.status.busy": "2021-01-26T09:18:18.077299Z",
     "iopub.status.idle": "2021-01-26T09:18:18.378980Z",
     "shell.execute_reply": "2021-01-26T09:18:18.379850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart.\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1.\n",
    "  True. Truncated backpropagation through time (TBPTT) is a modified version of the standard backpropagation algorithm that is used to train recurrent neural networks on long sequences of data. The modification involves breaking up the sequence into shorter subsequences of length S and performing forward and backward propagation on each subsequence separately. This is necessary to avoid the vanishing or exploding gradient problem that occurs in RNNs when gradients are backpropagated over many time steps.\n",
    "\n",
    "2. \n",
    "  False. While limiting the length of the sequence to S is one way to implement TBPTT, it is not the only way. Another approach is to use a sliding window to create overlapping subsequences of length S and perform forward and backward propagation on each subsequence. This allows the model to learn from longer sequences without the need to truncate them.\n",
    "\n",
    "3.\n",
    "  True. TBPTT allows the model to learn relations between input that are at most S timesteps apart. This is because the model is trained on subsequences of length S, and the gradients from each subsequence are used to update the model's parameters. As a result, the model can learn to recognize patterns in the input that occur within a window of S timesteps, but it may not be able to capture longer-term dependencies between inputs that are further apart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1.\n",
    "  The addition of attention in machine translation allows the model to selectively attend to different parts of the input sequence when generating the output sequence. In the absence of attention, the encoder processes the entire input sequence into a fixed-length vector, which is then passed to the decoder to generate the output sequence. This means that the decoder only has access to the encoder's final hidden state, which may not contain all the relevant information needed to generate the output sequence, especially for longer sequences.\n",
    "\n",
    "  Example:\n",
    "   Let's say we have a machine translation model that translates English sentences to French. Without attention, the encoder only generates a single fixed-size hidden representation for the entire input sentence, which is then used by the decoder to generate the output sequence. However, this representation may not capture all the relevant information in the input sequence, especially if the input sentence is long. By adding attention, the decoder can dynamically focus on different parts of the input sequence at each decoding step, giving it access to more fine-grained information about the input. This allows the encoder and decoder to learn more complex and accurate representations of their respective languages, resulting in better translation performance.\n",
    "\n",
    "  However, with attention, the decoder has access to all the encoder hidden states, and can selectively focus on the most relevant parts of the input sequence when generating each output token. This is done by computing a set of attention weights that determine the importance of each encoder hidden state for the generation of the current output token. By using the attention mechanism, the decoder can selectively attend to different parts of the input sequence, improving the quality of the generated output sequence.\n",
    "\n",
    "2. \n",
    "  In self-attention, the keys, queries, and values are all derived from the same input sequence, allowing the model to attend to different parts of the input sequence in a more flexible and powerful way. Specifically, in the case of machine translation, we can replace the queries that are typically the decoder hidden states with the encoder hidden states. This means that each decoder hidden state can attend to all the encoder hidden states, not just the last hidden state.\n",
    "  \n",
    "  Example:\n",
    "   Let's say we have a sentiment analysis model that classifies movie reviews as positive or negative. In the original model, the input sequence is processed by a bidirectional LSTM encoder to generate hidden representations for each token. These representations are then used as queries by the decoder (a single layer MLP) to generate the final classification score. However, by using self-attention instead, the queries, keys, and values are all equal to the encoder's hidden representations, which allows the model to attend to different parts of the input sequence depending on the context. This can improve the model's ability to capture long-range dependencies and improve its performance on tasks where context is important, such as natural language understanding.\n",
    "\n",
    "  By using self-attention in this way, we expect the model to learn more comprehensive representations of the input sequence, as each decoder hidden state will have access to all the encoder hidden states with learned projections. This allows the decoder to better capture the relationships between the different parts of the input sequence, potentially improving the quality of the generated output sequence. However, this approach may also be more computationally expensive, as each decoder hidden state has to attend to all the encoder hidden states, which can be a large number for longer sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "A.\n",
    " If the KL-divergence term is not included during training, the VAE's latent space will be under-constrained, and the model will tend to produce latent vectors that are distributed uniformly across the latent space. This, in turn, will lead to poor-quality reconstructions during training. The reconstructed images will lack the fine details and subtle variations that are present in the original images. Moreover, the VAE's latent space will not have a well-defined structure, making it difficult to use the model for tasks such as image generation and manipulation.\n",
    "\n",
    "B.\n",
    " If the KL-divergence term is not included during training, the VAE's latent space will be under-constrained, and the model will tend to generate images that are of poor quality and lack diversity. The generated images will be blurry and lack fine details and subtle variations, making them appear artificial and unrealistic. Moreover, the VAE's latent space will not have a well-defined structure, making it difficult to control the generative process and manipulate the generated images.\n",
    "======================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight.\n",
    "\n",
    "\n",
    "======================================================================\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "1)False.\n",
    " While the VAE's loss function encourages the latent space to be close to a standard normal distribution, this does not guarantee that the latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$. The actual distribution depends on the specific encoder used and the input image.\n",
    "\n",
    "2)False.\n",
    " The VAE's encoder maps an input image to a distribution over the latent space, rather than a single point in the latent space. Therefore, if we feed the same image to the encoder multiple times, we will get different distributions over the latent space each time. As a result, the decoder will produce different reconstructions each time, even though the input images are the same.\n",
    " \n",
    "3)True. \n",
    "The actual VAE loss term is intractable, so we use a lower bound on this loss, called the evidence lower bound (ELBO), that is tractable and can be optimized using stochastic gradient descent. Specifically, the ELBO consists of a reconstruction loss term and a KL-divergence term between the distribution over the latent space generated by the encoder and a prior distribution. While we want to minimize the actual loss term, we can only minimize the ELBO instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images.\n",
    "  \n",
    "\n",
    "======================================================================\n",
    "ANSWER:\n",
    "\n",
    "A.\n",
    "  True.\n",
    "  In a GAN, the generator's objective is to create images that are similar to the training data, while the discriminator's objective is to distinguish between real and fake images. As a result, a low generator loss means that the generator is producing images that are more realistic, and a high discriminator loss means that the discriminator is being fooled by the generator. Therefore, in order to train a GAN effectively, we need to balance the generator and discriminator losses, and ideally, we want a low generator loss and a high discriminator loss. For example, if the generator produces images that look nothing like the training data, then the discriminator loss will be low because it can easily distinguish between the real and fake images.\n",
    "\n",
    "B.\n",
    "  True. \n",
    "  In GAN training, the generator and discriminator are trained alternately. During each training iteration, the generator generates fake images, which are then fed into the discriminator along with real images. The discriminator then computes a loss that measures how well it can distinguish between the real and fake images. To improve the generator's performance, we need to update its weights based on the feedback from the discriminator. This means that we need to backpropagate the discriminator's loss through the generator and update its weights accordingly. Therefore, it is crucial to backpropagate into the generator when training the discriminator.\n",
    "\n",
    "C.\n",
    "  True.\n",
    "   In a GAN, the generator takes a random noise vector as input and generates a new image from it. This random noise vector is typically drawn from a normal distribution with mean zero and variance one (i.e., $\\mathcal{N}(\\vec{0},\\vec{I})$). While it is not a requirement to use this specific distribution, it is a common choice in GANs.\n",
    "\n",
    "D.\n",
    "  True. \n",
    "  In the early stages of training, the discriminator may produce arbitrary or inconsistent outputs, making it difficult for the generator to learn how to produce plausible images. By training the discriminator for a few epochs first, we give it a chance to learn some basic features of the training data and produce more reliable feedback to the generator.\n",
    "\n",
    "E.\n",
    "  False. \n",
    "  If the discriminator reaches a stable state where it has 50% accuracy, it means that it is effectively guessing whether each image is real or fake. In this case, further training of the generator is unlikely to improve the generated images significantly. In fact, it may lead to overfitting and a decrease in overall image quality. Instead, we would want to improve the discriminator's ability to distinguish between real and fake images, which would force the generator to produce more realistic images in response.\n",
    "======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "58d6fbcf83b8233a3f6425b3bd9ad41c0855f555d87100be9563bc1521748ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
